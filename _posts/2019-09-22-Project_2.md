---
layout: post
title: Second Project - Predicting Riyadh House Prices Using Linear Regression
---
## Project Goal:

My goal in this project was to use linear regression in order to estimate the sale price of a house in Riyadh, Saudi Arabia by 
using eSimsar website database.


### Look at the **eSimsar website**
Click the Link: [**eSimsar**](https://www.esimsar.com/)


## Project Overview:
In this project, I am going to develop and evaluate the performance and the predictive power of a model trained and tested on data collected from [**eSimsar**](https://www.esimsar.com/). Once I get an excellent model fit, I am going to use this model to predict the monetary value of a house located in Riyadh's area. 


## Project Approach:

These are the steps that I followed to achieve my project goal:-

### Web Scraping:

In this first step, I extract the page data of [**eSimsar**](https://www.esimsar.com/) website. I extracted 2662 rows and this is the code used for web scraping the eSimsar pages. 
```
list_url=[]
for x in range (1,266):
    url = 'https://www.esimsar.com/ar/search?c=1&ob=mr&page='+str(x)+'&q=%D8%A7%D9%84%D8%B1%D9%8A%D8%A7%D8%B6&t=35'
    response = requests.get(url)
    list_url.append(response.text)
    
size = []
bathroom = []
bed = []
price = []
neighborhood = []
region = []

for i in list_url:
    page = i
    soup = BeautifulSoup(page, "lxml")
    soup.prettify()
    for j in range(0,25):
        size.append(soup.find_all(class_='card__content')[j].find_all('p',attrs={'class':"card__property-amenity     card__property-amenity--area"})[0].text.strip())
        try:
            bathroom.append(soup.find_all(class_='card__content')[j].find_all('p',attrs={'class':"card__property-amenity card__property-amenity--bathrooms"})[0].text.strip())
        except:
            bathroom.append(int(0))
        try:
            bed.append(soup.find_all(class_='card__content')[j].find_all('p',attrs={'class':"card__property-amenity card__property-amenity--bedrooms"})[0].text.strip())
        except:
            bed.append(int(0))
        price.append(soup.find_all(class_='card__content')[j].find_all(class_='card__price-value')[0].text.strip())
        neighborhood.append(soup.find_all(class_='card__content')[j].find_all(class_='card__header')[0].find_all(class_='card__location')[0].text.split(',')[0].strip())
        region.append(soup.find_all(class_='card__content')[j].find_all(class_='card__header')[0].find_all(class_='card__location')[0].text.split(',')[1].strip())

```

After I extracted the data, I saved it in csv file. The below figure shows the information that I extracted from the pages.

![Image test]({{ site.url }}/images/house1.png)



### Cleaning Data & Exploratory Data Analysis:



### Visualising The Data & Correlation Matrix:


### Developing Models:


#### Baseline Model:


#### First Model:


#### Second Model:


#### Third Model:


#### Testing Final Model:



### Normalising The Price and House area (Size):



## Conclusions and Future Work:







