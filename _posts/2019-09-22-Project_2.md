---
layout: post
title: Second Project - Predicting Riyadh House Prices Using Linear Regression
---
## Project Goal:

My goal in this project was to use linear regression in order to estimate the sale price of a house in Riyadh, Saudi Arabia by 
using eSimsar website database.


### Look at the **eSimsar website**
Click the Link: [**eSimsar**](https://www.esimsar.com/)


## Project Overview:
In this project, I am going to develop and evaluate the performance and the predictive power of a model trained and tested on data collected from [**eSimsar**](https://www.esimsar.com/). Once I get an excellent model fit, I am going to use this model to predict the monetary value of a house located in Riyadh's area. 


## Project Approach:

These are the steps that I followed to achieve my project goal:-

### Web Scraping:

In this first step, I extracted the page data of [**eSimsar**](https://www.esimsar.com/) website. I extracted 2662 rows and this is the code used for web scraping the eSimsar pages. 
```
list_url=[]
for x in range (1,266):
    url = 'https://www.esimsar.com/ar/search?c=1&ob=mr&page='+str(x)+'&q=%D8%A7%D9%84%D8%B1%D9%8A%D8%A7%D8%B6&t=35'
    response = requests.get(url)
    list_url.append(response.text)
    
size = []
bathroom = []
bed = []
price = []
neighborhood = []
region = []

for i in list_url:
    page = i
    soup = BeautifulSoup(page, "lxml")
    soup.prettify()
    for j in range(0,25):
        size.append(soup.find_all(class_='card__content')[j].find_all('p',attrs={'class':"card__property-amenity     card__property-amenity--area"})[0].text.strip())
        try:
            bathroom.append(soup.find_all(class_='card__content')[j].find_all('p',attrs={'class':"card__property-amenity card__property-amenity--bathrooms"})[0].text.strip())
        except:
            bathroom.append(int(0))
        try:
            bed.append(soup.find_all(class_='card__content')[j].find_all('p',attrs={'class':"card__property-amenity card__property-amenity--bedrooms"})[0].text.strip())
        except:
            bed.append(int(0))
        price.append(soup.find_all(class_='card__content')[j].find_all(class_='card__price-value')[0].text.strip())
        neighborhood.append(soup.find_all(class_='card__content')[j].find_all(class_='card__header')[0].find_all(class_='card__location')[0].text.split(',')[0].strip())
        region.append(soup.find_all(class_='card__content')[j].find_all(class_='card__header')[0].find_all(class_='card__location')[0].text.split(',')[1].strip())

```

After I extracted the data, I saved it in csv file. The below figure shows the information that I extracted from the pages.

![Image test]({{ site.url }}/images/house1.png)



### Cleaning Data & Exploratory Data Analysis:

In the second step, I started to cleaning the data. I did some cleaning and filtering on the columns. I made the size column of the houses on the range between 200 and 1300, and the price column to be in range between 200,000 and 20,000,000. Also, since my data are not that big I deleted the neighborhood column and I added a new column called Region_Eng which is same as Region but translated to English. After the data cleaning, The below figure shows the remaining data.

![Image test]({{ site.url }}/images/house2.png)


### Visualising The Data & Correlation Matrix:


### Developing Models:


#### Baseline Model:


#### First Model:


#### Second Model:


#### Third Model:


#### Testing Final Model:



### Normalising The Price and House area (Size):



## Conclusions and Future Work:







